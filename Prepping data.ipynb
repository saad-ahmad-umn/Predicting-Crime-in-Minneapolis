{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import copy\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "from sagemaker import get_execution_role\n",
    "import pandas as pd\n",
    "import tweepy as tw\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "from datetime import datetime as dt\n",
    "\n",
    "## Make sure to **! pip install tweepy** before importing it\n",
    "\n",
    "import tweepy as tw\n",
    "\n",
    "# bucket_crime ='mpls-crime-data' # Our s3 bucket name\n",
    "data_key = 'all_data_for_prediction.csv'\n",
    "# data_location = 's3://{}/{}'.format(bucket_crime, data_key)\n",
    "# data = pd.read_csv(data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract year, month and day to make a date column. This comes in handy for the joins with the twitter dataset (and later when we split data into train and test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['full_date'] = pd.to_datetime(data[['Year', 'Month', 'Day']])\n",
    "\n",
    "data[\"week\"] = data[\"full_date\"].dt.week\n",
    "data[\"dayofweek\"] = data[\"full_date\"].dt.dayofweek # sunday=0\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['prev_year'] = data.groupby([data['full_date'].dt.year,data['full_date'].dt.week, \n",
    "             data['full_date'].dt.weekday, data['Neighborhood']])['count_incidents'].shift()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authorizing Twitter with the access tokens and consumer keys\n",
    "\n",
    "Accessing the Twitter API requires creating a developer account, i.e. essentially telling Twitter that you're an app developer and you would be requiring access to the API for fetching data. This process typically takes a day and requires you to justify your need for using the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = \"2572984207-WDAs0bPwMsrybwxX8RFGNqugeQeUKpu6sBIWbqa\"\n",
    "access_token_secret = \"cWnQlEF5m0zCdRcpOpFqtJVa7S9rG21zJiU2gZG9LsNun\"\n",
    "consumer_key = \"7XWcRRoNdd1WegVN20wAdUMG4\"\n",
    "consumer_secret = \"nxeXbBBnJoSf9cHA0Yv70cbCzPxHPNIsAmc49S0NfDoVdWwk8A\"\n",
    "\n",
    "\n",
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tw.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using user handles of news agencies to retrieve tweets \n",
    "\n",
    "We use the local news agencies here in the twin cities to gather tweets for a given day. Given the limitations of the free API, we chose to use news agencies and not users (since an average regular user would not be tweeting as much and we had a limitation on how far back we can go historically)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = [\"PioneerPress\",\n",
    "        \"StarTribune\",\n",
    "        \"WCCO\",\n",
    "        \"KSTP\",\n",
    "        \"FOX9\",\n",
    "        \"MPRnews\",\n",
    "        \"Jacob_Frey\",\n",
    "        \"melvincarter3\",\n",
    "        \"MnDPS_MSP\",\n",
    "        \"kare11\",\n",
    "        \"TCCrimeWatch\",\n",
    "        \"CrimeStoppersMN\",\n",
    "         \"UMNews\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hit the *user_timeline* end-point of the twitter API and put a filter on the number of days since when we want the data.\n",
    "\n",
    "(I would like to mention here that even though the API mentions a cap of 7-14 days, we were able to get data dating back till the beginning of this year, even though it was very sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = []\n",
    "for u in users:\n",
    "    tweets_user_1 = tw.Cursor(api.user_timeline,\n",
    "                   screen_name=u,since=date_since).items(2000)\n",
    "    time.sleep(3)\n",
    "    x = [[tweet.user.screen_name, tweet.text, tweet.created_at] for tweet in tweets_user_1]\n",
    "    all_tweets.append(x)\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parsing only the user name, text and the tweet time of the tweet\n",
    "all_tweets = [[tweet.user.screen_name, tweet.text, tweet.created_at] for tweet in tweets_user_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Making a dataframe out of the list of lists\n",
    "data = []\n",
    "for tw in all_tweets:\n",
    "    for i in enumerate(tw):\n",
    "        data.append(i[1])\n",
    "        \n",
    "tweets_data=pd.DataFrame(data=data, columns=['User','Tweet','Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## printing a sample of the dataset and checking the number of rows\n",
    "\n",
    "print(tweets_data.head())\n",
    "print()\n",
    "print(tweets_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Amazon comprehend client\n",
    "\n",
    "boto3 is the package using which we can access all of AWS's APIs (in this case we hit AWS Comprehend's Sentiment analysis API). We define an empty list called `resp` which will store the reponses (each response contains a weight corresponding to how Positive/Negative/Neutral/Mixed the tweet is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "client = boto3.client('comprehend')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp=[]\n",
    "## using lambda function with apply to apply sentiment analysis to each tweet in the dataframe\n",
    "resp.append(tweets_data['Tweet'].apply(lambda x: client.detect_sentiment(Text=x, LanguageCode='en')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'resp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-849a9e2ef516>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mneutral\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mitems\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mpositive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SentimentScore'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Positive'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'resp' is not defined"
     ]
    }
   ],
   "source": [
    "## we create a separate list for each score since we would be making a column out of each. The sentiment column is essentially\n",
    "## the most prevailing sentiment of the day\n",
    "\n",
    "label = []\n",
    "positive = []\n",
    "negative = []\n",
    "mixed = []\n",
    "neutral = []\n",
    "\n",
    "for items in resp[0]:\n",
    "    label.append(items['Sentiment'])\n",
    "    positive.append(items['SentimentScore']['Positive'])\n",
    "    negative.append(items['SentimentScore']['Negative'])\n",
    "    mixed.append(items['SentimentScore']['Mixed'])\n",
    "    neutral.append(items['SentimentScore']['Neutral'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_data['Sentiment'] = label\n",
    "tweets_data['PositiveWeight'] = positive\n",
    "tweets_data['NegativeWeight'] = negative\n",
    "tweets_data['MixedWeight'] = mixed\n",
    "tweets_data['NeutralWeight'] = neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime as dt\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "## Converting Date column from string to Datetime format\n",
    "tweets_data['Date'] = tweets_data['Date'].apply(lambda x: dt.strptime(x, '%m/%d/%Y %H:%M').date())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are interested in the average sentiment of a day, we take a mean of all scores and roll-up the data to a daily level. Multiple other methods can be tried, for e.g. - \n",
    "\n",
    "* If you feel 2 highly positive tweets should weigh less than 10 moderately positive tweets, you can multiple them by their count (weighted average)\n",
    "* Conversely, if you feel strongly weighted tweets (albeit less in number) should be more influential, multiply the sentiment by the inverse of count, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets_data_agg = tweets_data.groupby('Date').agg({'PositiveWeight':['mean','count'], 'NegativeWeight':['mean','count'], \n",
    "                                 'NeutralWeight':['mean','count'], 'MixedWeight':['mean','count']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in the crimes datafile and rolling up to a precinct-day level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reading in training datafile\n",
    "data_file = 'rik_prediction.csv'\n",
    "data_location = 's3://{}/{}'.format(bucket_crime, data_file)\n",
    "full_train = pd.read_csv(data_location)\n",
    "crime_counts = full_train.groupby(['ReportedDate','Precinct']).agg({'count_incidents':'sum'}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in weather data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading in weather data\n",
    "data_file = 'mpls_weather_data_2017-2019_12_07.csv'\n",
    "data_location = 's3://{}/{}'.format(bucket_crime, data_file)\n",
    "weather = pd.read_csv(data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in dashboard data-file\n",
    "\n",
    "This is a data file that contains data at a date, neighborhood and crime type level. This was utilized, by joining with the weather dataset, to generate the plots on the map for our dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading in crime data for dashboard\n",
    "data_file = 'dashboard.csv'\n",
    "data_location = 's3://{}/{}'.format(bucket_crime, data_file)\n",
    "dashboard = pd.read_csv(data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'weather' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b59c78e7c81a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Converting all string dates to datetime format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mweather\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DATE'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweather\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DATE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'%Y-%m-%d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdashboard\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reportedDateTime'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdashboard\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reportedDateTime'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'%Y-%m-%d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'weather' is not defined"
     ]
    }
   ],
   "source": [
    "## Converting all string dates to datetime format\n",
    "weather['DATE'] = weather['DATE'].apply(lambda x: dt.strptime(x,'%Y-%m-%d').date())\n",
    "dashboard['reportedDateTime'] = dashboard['reportedDateTime'].apply(lambda x: dt.strptime(x,'%Y-%m-%d').date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dashboard.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Taking only relevant columns\n",
    "weather = weather[['PRCP','SNOW','SNWD','TAVG','DATE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## weather and crime merging for dashboard\n",
    "dashboard_all_data = pd.merge(dashboard, weather, left_on='reportedDateTime', right_on='DATE', how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write dataframe to buffer\n",
    "csv_buffer = StringIO()\n",
    "dashboard_all_data.to_csv(csv_buffer, index=False)\n",
    "\n",
    "# Upload CSV to S3\n",
    "s3_key = 'data_for_dashboard.csv'\n",
    "s3_resource = boto3.resource('s3')\n",
    "s3_resource.Object(bucket_crime, s3_key).put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting string to date - temp is the dataframe that contains crime count data at a daily-precinct level\n",
    "crime_counts['ReportedDate'] = crime_counts['ReportedDate'].apply(lambda x: dt.strptime(x,'%Y-%m-%d').date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## getting weather and crime data into a single dataframe\n",
    "full_train = pd.merge(weather, crime_counts, left_on='DATE',right_on='ReportedDate',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Since the columns were grouped (as mean and count for each type of sentiment, we ungroup and create a named column for\n",
    "## each of them as ** sentiment_metric **)\n",
    "\n",
    "l0_cols = tweets_data_agg.columns.get_level_values(0)\n",
    "l1_cols = tweets_data_agg.columns.get_level_values(1)\n",
    "temp_cols = list(zip(l0_cols, l1_cols))\n",
    "colnames = [i[0]+'_'+i[1] for i in a]\n",
    "tweets_data_agg.columns = colnames\n",
    "\n",
    "tweets_data_agg.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we merge the tweets dataset with the full-train dataset (that contains weather and crime data at a daily-precinct level)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_train['ReportedDate_formatted'] = full_train['ReportedDate'].apply(lambda x: dt.strptime(x,'%Y-%m-%d').date())\n",
    "final_dataset = pd.merge(full_train, tweets_data_agg, left_on='ReportedDate', right_on = 'Date', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing to S3 bucket\n",
    "\n",
    "Now that we are done preparing the dataset (containing crimes, weather and sentiment data, from tweets) we write it out to an S3 location. This dataset is used in the prediction notebook to run our Xgboost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Write dataframe to buffer and then write it out to an S3 location\n",
    "\n",
    "csv_buffer = StringIO()\n",
    "final_dataset.to_csv(csv_buffer, index=False)\n",
    "\n",
    "# Upload CSV to S3\n",
    "s3_key = 'data_with_sentiment.csv'\n",
    "s3_resource = boto3.resource('s3')\n",
    "s3_resource.Object(bucket_crime, s3_key).put(Body=csv_buffer.getvalue())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
